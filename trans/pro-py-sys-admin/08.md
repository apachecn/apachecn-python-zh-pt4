# 八、Nagios 的网站可用性检查脚本

在本章中，我们将为目前可用的标准网络监控系统(NMS)之一 Nagios 构建一个定制的检查脚本。我们将通过使用 HTML 解析库来监控一个简单的网站，它允许我们检查网站的操作方面。检查脚本试图通过模拟用户登录操作，浏览不安全的页面，然后到达一些受保护的页面。所有的动作都将被记录并反馈到 Nagios 系统，该系统可以被配置为进行报告和报警。如果需要。

对检查系统的要求

我们将要实现的系统的主要需求是监控远程网站的能力。然而，检查应该超越简单的 HTTP GET 或 POST 请求，它必须允许用户指定导航路径。例如，它应该能够执行一些模拟标准用户行为的操作:进入主网站页面，然后浏览产品列表或导航到新闻网站并选择头条新闻。

作为该场景的变体，系统还需要能够模拟一个登录过程，通过该过程，支票将用户详细信息提交给远程网站。然后，系统会验证这些详细信息，并返回安全令牌(通常以浏览器 cookie 值的形式)。

与简单的 HTTP 检查不同，默认的 Nagios 发行版提供了简单的 HTTP 检查，这种机制实际上触发了 web 应用逻辑，并充当更复杂的检查。当它与计时参数结合使用时，可以实现复杂的检查，监视用户登录时间，并在登录过程成功但耗时过长时发出警报。

我们将使用 Python 的标准 urllib 和 urllib2 库来访问网站。作为一个网页解析器，我们要用漂亮的 Soup HTML 解析库。

每个网站都是独一无二的，或者至少是试图脱颖而出。因此，制作通用支票系统可能是一项复杂的任务；为了简单起见，我将对我们将在本章中构建的系统设置一些约束。例如:

*   导航(或用户旅程)路径将在脚本中编码，不作为配置提供。
*   登录检查仅适用于使用基于 cookie 的身份验证机制的站点。

Nagios 监控系统

Nagios 是最流行的网络监控系统之一。它用于监控使用不同访问协议(如 HTTP、SNMP、FTP 和 SSH)的各种网络连接组件。这些功能是无穷无尽的，因为 Nagios 有一个基于插件的架构，允许您扩展基本功能以满足您的监控需求。您也可以使用 Nagios 远程插件执行器(NRPE)实用程序远程运行检查。

除了监视任务之外， Nagios 还能够将收集到的数据图形化，比如系统响应时间或 CPU 利用率。当出现问题时，Nagios 能够通过电子邮件或短信通知发出警报。

Nagios 包(基础应用和插件)在大多数 Linux 平台上都是可用的，所以请查看您的 Linux 发行版文档以了解安装细节。或者，您可以从 Nagios 网站的[www.nagios.org/download](http://www.nagios.org/download)下载源代码。在典型的 Fedora 系统上。您可以使用以下命令安装 Nagios 基本系统以及一组基本的插件(或检查):

```py
$ sudo yum install nagios nagios-plugins-all
```

在 Debian 系统上，您可以运行以下命令:

```py
$ sudo apt-get install nagios3
```

要继续学习本章，您应该有一些管理 Nagios 的经验。如果您需要更多信息，请参考您可以在[www.nagios.org/documentation/](http://www.nagios.org/documentation/)网站在线找到的官方文档。

Nagios 插件架构

Nagios NMS 的强大之处在于它的插件架构。所有的检查命令都是外部实用程序，可以用任何语言编写——C、Python、Ruby、Perl 等等。插件通过操作系统返回代码和标准输入/输出机制与 Nagios 系统通信。换句话说，Nagios 有一组预定义的返回代码，检查脚本必须返回这些代码。返回代码指示新的服务状态应该设置为什么。所有返回代码和相应的服务状态在[表 8-1](#Tab1) 中列出。

[表 8-1](#_Tab1) 。Nagios 插件返回代码

| 

返回代码

 | 

服务状态

 |
| --- | --- |
| Zero | 好的。这项服务状况良好。 |
| one | 警告。服务是可用的，但是危险地接近危急状况。 |
| Two | 危急关头。该服务不可用。 |
| three | 未知。无法确定服务的状态。 |

除了返回代码之外，插件应该在标准输出中至少打印一行。这个打印的字符串应该包含一个强制的状态文本，后跟可选的性能数据字符串。因此，一个简单的单行报告示例可以是:

```py
WebSite OK
```

该文本将被附加到 Nagios GUI 中的状态报告消息中。类似地，附加了性能数据后，它看起来像这样:

```py
WebSite OK | response_time=1.2
```

性能数据部分可以通过内置的 Nagios 宏获得，并可以用于绘制图表。有关使用性能数据参数的更多信息，请访问[http://nagios.sourceforge.net/docs/3_0/perfdata.html](http://nagios.sourceforge.net/docs/3_0/perfdata.html)。

当您编写一个新的插件时，您必须首先在配置文件中提供它，以便 Nagios 知道在哪里可以找到它。按照惯例，所有插件都存储在/usr/lib/nagios/plug-ins 中。

一旦编写了检查脚本，就必须在 command.cfg 配置文件中定义它，该文件可以在/etc/nagios/objects/中找到。实际位置可能不同，这取决于您如何安装 Nagios。以下是支票定义的示例:

```py
define command {
        command_name    check_local_disk
        command_line    $USER1$/check_disk -w $ARG1$ -c $ARG2$ -p $ARG3$
}
```

当您定义服务或主机时，现在可以使用 check_local_disk 名称来引用此检查。实际的可执行文件是$USER1$/check_disk，接受三个参数。下面是一个服务定义的示例，该服务定义使用此检查并将所有三个参数传递给它:

```py
define service {
        use                             local-service
        host_name                       localhost
        service_description             Root Partition
        check_command                   check_local_disk!10%!5%!/
}
```

您之前在命令行定义中看到的＄user 1＄宏只是指插件目录，在/etc/Nagios/private/resource . CFG 中定义为＄user 1 ＄=/usr/lib/Nagios/plugins。

如果需要，您可以定义一个新宏，并将其用于您的检查脚本。通过这种方式，您可以将打包的脚本与您自己的脚本分开，这样维护起来就更容易了。我建议对具有复杂结构的检查脚本这样做，这些脚本带有外部配置文件或其他依赖项。

网站导航检查

如您所知，每个网站都是独一无二的，尽管通常应用相同的导航和实现原则，但您仍然需要做大量的手工工作来对其进行逆向工程，以便您可以成功地模拟用户操作。如果你知道网站是如何建立的，事情就变得简单多了，不需要猜测。在这个示例检查中，我们将构建一个脚本，该脚本导航到位于[http://news.bbc.co.uk/](http://news.bbc.co.uk/)的 BBC 英国网站，选择头条新闻，并跟随该链接。

这个检查是一个很好的例子，它模拟了一个用户行为模式，还测试了至少两个功能的内部网站逻辑:生成首页的能力和生成头条内容的能力。我们还将监控执行时间，如果它超过了预先配置的阈值，我们也会发出警报。

安装漂亮的 Soup HTML 解析库

在继续之前，我们需要安装漂亮的汤库。Beautiful Soup 是一个 Python 模块，用于解析 HTML 和 XML 文档，并从中提取信息。这个库非常适合处理现实世界中的 HTML 页面，因为它忽略了格式错误的 HTML 语法，缺少结束标记以及网页可能包含的其他错误。

因为 Beautiful Soup 是一个非常受欢迎的库，它的包适用于大多数 Linux 发行版。例如，在 Fedora 系统上，我们可以使用以下命令安装这个库:

```py
$ sudo yum install python-BeautifulSoup
```

我们也可以从 Python 包索引(PyPI)安装它:

```py
$ sudo pip install BeautifulSoup
```

或者，可以从位于[www.crummy.com/software/BeautifulSoup/](http://www.crummy.com/software/BeautifulSoup/)的应用网站下载源代码。

检索网页

在其最简单的形式中，页面检索功能只需要两次函数调用就可以实现，在大多数情况下，我们不提交任何信息，只进行检索，这就足够了。以下示例使用 urlopen()函数，如果没有提供额外的表单数据，该函数将执行 HTTP GET 请求。在本章的后面，我们将会看到向 web 应用提交数据的不同方法。

```py
>>> import urllib2
>>> r = urllib2.urlopen('http://news.bbc.co.uk')
>>> html = r.read()
>>> len(html)
143484
```

read()调用的结果是一个字符串，其中包含服务器提供的网页。但是，该字符串不是完整的响应，并且不包括额外的信息，如 HTTP 协议头。urlopen()调用返回的结果对象有 info()方法，我们可以用它来检索服务器返回的 HTTP 头。我们需要记住 info()调用返回的对象是 httplib 的一个实例。HTTPMessage 类，它实现了与 dictionary 类相同的协议，但实际上它本身并不是一个字典:

```py
>>>r.info()
<httplib.HTTPMessage instance at 0x1005c7ef0>
>>>print r.info()
Server: Apache
X-Cache-Action: HIT
X-Cache-Hits: 133
Vary: X-CDN
X-Cache-Age: 8
Cache-Control: private, max-age=0, must-revalidate
Content-Type: text/html
Date: Tue, 13 May 2014 18:02:06 GMT
Expires: Tue, 13 May 2014 18:01:58 GMT
Content-Language: en-GB
X-LB-NoCache: true
Connection: close
Set-Cookie: BBC-UID=758377d205ee016ea1140d3d4136ec148f4d29ac7484e1befa21640e52188e380Python-urllib/2.7; 
expires=Sat, 12-May-18 18:02:06 GMT; path=/; domain=.bbc.co.uk
Content-Length: 143484

>>>r.info()['Server']
'Apache'
>>>
```

![Image](img/00010.jpeg) **提示**你可以在[www.cs.tut.fi/~jkorpela/http.html](http://www.cs.tut.fi/~jkorpela/http.html)找到更多关于 HTTP 头的信息，包括简短的描述和到适当 RFC 规范文档的链接。

响应对象的另一个有用的方法是 geturl()。这个方法返回被检索文档的实际 URL。初始 URL 可能会响应 HTTP 重定向，实际上我们最终会从一个完全不同的 URL 检索到一个页面。在这种情况下，我们可能需要检查页面的来源。重定向的一个可能原因是，我们试图在没有事先验证的情况下访问受限制的内容。在这种情况下，我们很可能会被重定向到网站的登录页面。

```py
>>> r.geturl()
'http://www.bbc.co.uk/news/'
```

产生的内容可以传递给 Beautiful Soup 进行 HTML 解释和解析。结果是一个 HTML 文档对象，它实现了从文档中搜索和提取数据的各种方法。提供给漂亮的 Soup 构造函数的参数只是一个字符串，这意味着我们可以使用任何字符串作为参数，而不仅仅是我们刚刚从网站上获取的那个。

```py
>>> from BeautifulSoup import BeautifulSoup
>>> soup = BeautifulSoup(html)
>>> type(soup)
<class 'BeautifulSoup.BeautifulSoup'>
>>>
```

用漂亮的汤解析 HTML 页面

一旦内容被加载到 BeautifulSoup 对象中，我们就可以开始剖析 BBC 首页了。为了让你知道我们需要在页面上找到什么，[图 8-1](#Fig1) 展示了一个首页截图的例子，我们可以清楚地看到头条的位置。当我捕捉到这个画面时，BBC 英国新闻的头条新闻是“法庭调查英国虐待伊拉克的指控”每篇文章的标题都有明显的变化，但网站的布局很少变化，头条新闻总是显示在网页的同一位置。

![9781484202180_Fig08-01.jpg](img/00032.jpeg)

[图 8-1](#_Fig1) 。英国广播公司英国新闻头版

我们现在需要在网页中找到相应的 HTML 代码。让我们来看看网页源代码，如清单 8-1 所示。(我做了一些格式化，所以如果您从 web 浏览器查看代码，您可能会看到稍微不同的代码布局。)

[***清单 8-1***](#_list1) 。BBC 新闻英国版首页的 HTML 源代码

```py
[...]

<div id="now" class="container-now">
  <div id="container-top-stories-with-splash" class="container-top-stories">
    <div id="top-story" class="large-image">
      <h2 class="top-story-header ">
        <a class="story" rel="published-1399992906414" href="/news/uk-27397695">
          Court to probe UK Iraq abuse claims<img src="http://news.bbcimg.co.uk/media/
          img/jpg/_74828076_74828067.jpg" alt="British soldiers" /></a>
      </h2>
      <p>An initial investigation into claims that UK forces abused Iraqi detainees is to be opened by the International Criminal Court.
<span id="dna-comment-count___CPS__27397695" class="gvl3-icon gvl3-icon-comment comment-count"></span>
      </p>
      <ul class="see-also">
        <li class=" first-child column-1">
          <a class="story" rel="published-1389534113695" href="/news/uk-25703723">Hague rejects Iraq 'abuse' complaint</a>
        </li>
        <li class=" column-1">
          <a class="story" rel="published-1380724487138" href="/news/uk-24361521">Military 'must aid Iraq inquiries'</a>
        </li>
        <li class=" column-2">
          <a class="story" rel="published-1363716369370" href="/news/uk-21740286">Q&amp;A: Al-Sweady inquiry</a>
        </li>
      </ul>
      <hr />
    </div>
    <div id="second-story" class="secondary-top-story">
      <div class="large-image">
        <h2 class=" secondary-story-header">
          <a class="story" rel="published-1399993960900" href="/news/world-europe-27396448"><img           src="http://news.bbcimg.co.uk/medimg/jpg/_74826634_74826454.jpg" alt="Pro-
          Russian militant in Donetsk - 13 May" />'Troops killed' in Ukraine ambush</a>
        </h2>
        <p>Seven Ukrainian soldiers and one pro-Russian insurgent have reportedly been killed in an 
         ambush in the eastern Donetsk region, reports say.
<span id="dna-comment-count___CPS__27396448" class="gvl3-icon gvl3-icon-comment comment-count"></span>
        </p>
        <ul class="see-also">
          <li class=" first-child column-1">
            <a class="story" rel="published-1399980282897" href="/news/world-europe-27392074">Ukrainian speakers leave Donetsk</a>
          </li>
          <li class=" column-1">
            <a class="story" rel="published-1399911372202" href="/news/business-27374070">Russia deadline on Ukraine gas debt</a>
          </li>
          <li class=" column-2">
            <a class="story" rel="published-1399904895703" href="/news/world-europe-27379219">Rosenberg: What will Putin do next?</a>
          </li>
          <li class=" column-2">
            <a class="story" rel="published-1399896081600" href="/news/world-europe-27376297">Media 
            mull Ukraine vote significance</a>
          </li>
        </ul>
      </div>
    </div>

[...]

```

我们可以立即发现三个不同的标记，它们可能会将我们引向顶层 URL 链接。第一个是 ID 设置为 top-story 的

标记。第二个标记是属于 top-story-header 类的

## 标记。最后一个标记可能是属于 tshsplash 类的

有几种方法可以访问漂亮的 Soup 文档中的标签。如果我们确切地知道我们正在寻找什么和网站的确切结构，我们可以简单地使用标签名称作为 soup 对象的属性:

```py
>>> import urllib2
>>> from BeautifulSoup import BeautifulSoup
>>> WEBSITE = 'http://www.bbc.co.uk/news/'
>>> result = urllib2.urlopen(WEBSITE)
>>> html = result.read()
>>> soup = BeautifulSoup(html)
>>> print soup.html.head.title
```

这段代码将打印标题 HTML 字符串:

```py
<title>BBC News - Home</title>
```

这是一种方便快捷的访问单个标签的方法，但是如果标签封装结构很复杂，这种方法就不能很好地工作。例如，我们试图到达的第一个

标签已经比封装它的标签深了九层(见[图 8-2](#Fig2) ，我们甚至不知道那个特定的< div >标签相对于文档的根元素在哪里。

![9781484202180_Fig08-02.jpg](img/00033.jpeg)

[图 8-2](#_Fig2) 。英国广播公司新闻英国头版 DOM 树(部分)

对于这种情况，Beautiful Soup 提供了 find 方法，允许我们搜索元素，而不管它们在文档树中的什么位置。换句话说，搜索是递归的。有两个 find 方法 : findAll，返回所有匹配搜索字符串的标签列表；和 find，它返回匹配标记的第一个匹配项。

另外，请记住，每个文档元素都实现了与主 soup 对象相同的搜索方法。因此，如果我们想要获得包含在另一个

中的每个，我们将首先通过其 ID 或其类搜索第一个标签，然后仅从该对象开始运行另一个搜索查询，如下所示:

```py
>>> top_div = soup.find('div', {'id': 'now'})
>>> divs = top_div.findAll('div')
>>> divs[-1]
<div class="languages-footer">
<a href="/worldservice/languages/">More languages</a>
</div>
>>>
```

对我们来说幸运的是，BBC 新闻网站结构良好，我们只需少量搜索就能获得头条新闻。首先，我们将使用 ID top-story 获取

。这让我们非常接近顶层链接。然后，我们将获得属于 top-story-header 类的

## 元素。产生的子树将包含所需的

```py
>>> top_story_div = soup.find('div', {'id': 'top-story'})
>>> h2_tag = top_story_div.find('h2', {'class': 'top-story-header '})
>>> a_tag = h2_tag.find('a', {'class': 'story'})
>>> a_tag
<a class="story" rel="published-1399992906414" href="/news/uk-27397695">
      Court to probe UK Iraq abuse claims<img src="http://news.bbcimg.co.uk/medimg/jpg/_74828076_74828067.jpg" alt="British soldiers" /></a>
>>>
```

检查字典键是否实际存在总是一个好主意；否则，您将会得到 KeyError 异常。但是，因为我们没有访问“真正的”Python 字典对象，所以我们不能使用 IS IN 构造，因为它会给我们一个不正确的结果:

```py
>>> 'href' in a_tag
False
>>> a_tag.has_key('href')
True
>>> a_tag['href']
u'/news/uk-27397695'
>>>
```

下一步是加载这个页面。只需成功加载此页面就足以确认网站正在运行，因此我们不会对该页面进行任何 HTML 解析。检查脚本还需要测量检索两个网页所花费的时间。如果时间超过了定义的阈值，脚本将返回一个错误代码。[清单 8-2](#list2) 显示了完整的校验码。

[***清单 8-2***](#_list2) 。站点导航脚本

```py
#!/usr/bin/env python

import sys
import urllib2
import time
from BeautifulSoup import BeautifulSoup
from optparse import OptionParser

NAGIOS_OK = 0
NAGIOS_WARNING = 1
NAGIOS_CRITICAL = 2
WEBSITE_ROOT = 'http://news.bbc.co.uk/'

def fetch_top_story():
    status = []
    try:
        result = urllib2.urlopen(WEBSITE_ROOT)
        html = result.read()
        soup = BeautifulSoup(html)
        a_tag = soup.find('a', 'tshsplash')
        story_heading = a_tag.string
        topstory_url = ''
        if a_tag.has_key('href'):
            topstory_url = "%s/%s" % (WEBSITE_ROOT, a_tag['href'])
        else:
            status = [NAGIOS_CRITICAL, 'ERROR: Top story anchor tag has no link']
        result = urllib2.urlopen(topstory_url)
        html = result.read()
        status = [NAGIOS_OK, story_heading]
    except:
        status = [NAGIOS_CRITICAL, 'ERROR: Failed to retrieve the top story']
    return status

def main():
    parser = OptionParser()
    parser.add_option('-w', dest='time_warn', default=1.8,
                      help="Warning threshold in seconds, default: %default")
    parser.add_option('-c', dest='time_crit', default=3.8,
                      help="Critical threshold in seconds, default: %default")
    (options, args) = parser.parse_args()
    if options.time_crit < options.time_warn:
        options.time_warn = options.time_crit

    start = time.time()
    code, message = fetch_top_story()
    elapsed = time.time() - start
    if code != 0:
        print message
        sys.exit(code)
    else:
        if elapsed < float(options.time_warn):
            print "OK: Top story '%s' retrieved in %f seconds" % (message, elapsed)
            sys.exit(NAGIOS_OK)
        elif elapsed < float(options.time_crit):
            print "WARNING: Top story '%s' retrieved in %f seconds" % (message, elapsed)
            sys.exit(NAGIOS_WARNING)
        else:
            print "CRITICAL: Top story '%s' retrieved in %f seconds" % (message, elapsed)
            sys.exit(NAGIOS_CRITICAL)

if __name__ == '__main__':
    main()
```

如您所见，该脚本接受两个可选参数，允许我们设置警告和临界条件的阈值。在将其部署为 Nagios 检查之前，让我们用各种设置测试检查脚本，只是为了触发所有可能的条件:

```py
$ ./check_website_navigation.py -w 1.2
OK: Top story 'Court to probe UK Iraq abuse claims' retrieved in 0.540721 seconds
$ echo $?
0
$ ./check_website_navigation.py -w 0.4
WARNING: Top story 'Court to probe UK Iraq abuse claims' retrieved in 0.556052 seconds
$ echo $?
1
$ ./check_website_navigation.py -c 0.4
CRITICAL: Top story 'Court to probe UK Iraq abuse claims' retrieved in 0.535464 seconds
$ echo $?
2
$
```

向 Nagios 系统添加新的检查

现在是时候在 Nagios 中提供这项检查并开始监控 BBC 新闻网站了。首先，我们将在命令列表文件中添加新的部分，它是/etc/nagios/objects/目录中的 commands.cfg。以下代码使检查可以在名称 check_website_navigation 下使用，并指示命令需要提供两个参数:

```py
define command {
    command_name    check_website_navigation
    command_line    $USER2$/check_website_navigation -w $ARG1$ -c $ARG2$
}
```

然后，我们需要创建一个至少包含主机和服务定义的配置文件。清单 8-3 显示了如何创建一个简单的配置文件，该文件定义了一个主机模板，然后主机从该模板继承基本设置。然后，该主机被放入一个单独的主机组。类似地，具有新的检查命令的服务被定义并分组到单独的服务组中。当我们在本章后面添加另一个检查时，我们将扩展这个配置。一旦我们创建了配置文件，我们就必须在 nagios.cfg 文件中添加一个指向这个配置文件的 cfg_file 语句。

[***清单 8-3***](#_list3) 。Nagios 主机和服务定义

```py
define host {
    name            template-website-host
    use            generic-host
    register        0
    max_check_attempts    5
    contacts        nagiosadmin
    parents            localhost
    check_command        check-host-alive
}

define host {
    use            template-website-host
    host_name        news.bbc.co.uk
    address            news.bbc.co.uk
    notes            BBC News UK
}

define hostgroup {
    hostgroup_name        InternetWebsites
    alias            Internet Websites
    members            news.bbc.co.uk
}

define service {
    use            generic-service
    hostgroup_name        InternetWebsites
    service_description    SiteNavigation
    check_command        check_website_navigation!1.5!2.5
}

define servicegroup {
    servicegroup_name    InternetWeb
    alias            Internet Websites
    members            news.bbc.co.uk,SiteNavigation
}
```

如果我们给 Nagios 一些时间来重新检查所有已定义的服务，然后导航到服务检查屏幕，我们应该会看到类似于图 8-3 所示的结果。

![9781484202180_Fig08-03.jpg](img/00034.jpeg)

[图 8-3](#_Fig3) 。检查 Nagios 中的脚本状态

模拟用户登录过程

我们要实现的下一个检查是用户登录操作。对于一个示例网站，我将使用[www.telegraph.co.uk/](http://www.telegraph.co.uk/)。该网站允许用户参与不同的促销活动，订阅邮件列表和电子邮件通知。显然，这些选项需要允许用户向网站表明自己的身份。

当用户单击位于网页右上角的登录链接时，他或她将被重定向到登录登录页面。该页面包含一个带有两个字段的 web 页面表单:一个用于用户电子邮件地址，另一个用于密码。[清单 8-4](#list4) 显示了网页源代码中的表单定义。

[***清单 8-4***](#_list4) 。telegraph.co.uk 登录表单

```py
<form id="user" class="basicForm" action="./login.htm" method="post">
  <label for="email" >Email address</label>

  <input id="email" name="email" type="text" value=""/>

  <label for="password" >Password</label>

  <input id="password" name="password" type="password" value="" autocomplete="off"/>

  <div class="cl"></div>
  <a href="forgotpassword.htm" class="noLabel">Forgotten password?</a><br/>
  <a href="http://www.telegraph.co.uk/topics/about-us/3489870/Contact-us.html"
class="noLabel">Need help?</a>
  <div class="bottomButtons">

    <input type="submit" value="Log in" />

  </div>
  <div class="cl"></div>
  <p class="noLabel">Or<a href="registration.htm">register now</a>
    if you do not have a Telegraph.co.uk profile
  </p>
</form>
```

当我们填写值并点击 Submit 按钮时，web 浏览器通过将所有字段(包括字段名及其新值)组合成一个字符串来对值进行编码，并将该信息作为 HTTP POST 请求发送。HTTP 方法通常在表单定义中指定，从我们的例子中可以看出，它当前被设置为 POST。

如果我们想要达到同样的结果，我们首先需要封装我们将要提交的数据。不幸的是，urllib2 不提供这种功能，我们必须使用 urllib 方法对表单数据进行编码。包含表单数据的格式化字符串应该作为可选参数提供给 urlopen()方法。如果提供了附加数据，该方法将自动发送 POST 请求，而不是默认的 GET 请求。

![Image](img/00010.jpeg) **注意**POST 和 GET 请求有什么区别？主要区别在于这两个请求向 web 服务提交额外数据的方式。如果发送 GET 请求，数据包含在 URL 字符串中。URL 的语法应该与此类似:[http://example.com/some_page](http://example.com/some_page)？key =值&key 2 =值 2。相反，如果发送 POST 请求，URL 将是[http://example.com/some_page](http://example.com/some_page)，数据将封装在 HTTP 请求头中。

网站通常使用 HTTP cookies 来管理用户会话。HTTP cookie 是一个协议消息字段，包含在从网络浏览器应用发送到网络服务器的通信消息中。HTTP 协议本质上是无状态的。HTTP 请求不携带任何有助于识别请求发送者的信息。然而，跟踪用户活动对于网络购物服务或任何其他需要提供个性化结果的服务来说是必不可少的。这项活动被称为“维护 web 会话”维护这个会话的方法之一是使用 HTTP cookies。下面是一个 HTTP cookie 的例子:

```py
Set-Cookie: BBC-UID=758377d205ee016ea1140d3d4136ec148f4d29ac7484e1befa21640e52188e380Python-urllib/2.7; expires=Sat, 12-May-18 18:02:06 GMT; path=/; domain=.bbc.co.uk
```

在 HTTP 头消息中可以设置多个 cookies。每个 cookie 都有一个名称、一个值和一些额外的属性，比如应该接收它的域、过期时间和 URL 部分。那么 cookies 是如何帮助维护会话的呢？当 web 服务器收到请求时，它会将初始响应发送回 web 浏览器。连同其他 HTTP 头字段一起，它插入了 cookie 字段。web 客户端依次将 cookie 保存在其内部数据库中。当它发出另一个请求时，它会扫描数据库，查找属于当前向其发送请求的同一个域并且具有匹配的 path 属性的 cookies。然后，web 客户端在其后续请求中包含所有匹配的 cookies。现在，web 服务器接收到用 cookies“标记”的请求，因此知道这些请求是同一个“会话”的一部分，或者换句话说，属于同一个 web 会话。

我已经描述了自动处理 cookie 存储和管理活动的典型 web 浏览器的行为。默认的 URL 处理器(或 urllib2 术语中的开启器)不处理 cookies。幸运的是，所有处理 cookies 的类都包含在 urllib2 模块中，您只需要用自定义的 opener 对象替换默认的 opener。我们将在构造新的 opener 对象时使用的 HTTPCookieProcessor 类负责存储从服务器接收到的 HTTP cookies，然后将它们注入到发往同一网站的所有 HTTP 请求中:

```py
>>> import urllib, urllib2
>>> url = 'https://auth.telegraph.co.uk/sam-ui/login.htm'
>>> data = urllib.urlencode({'email': 'user@example.com', 'password': 'secret'})
>>> opener = urllib2.build_opener(urllib2.HTTPCookieProcessor())
>>> urllib2.install_opener(opener)
>>> result = opener.open(url, data)
>>> html = result.read()
>>> print html

[...]

  <head>

    <title>My Account</title>

  </head>

[...]
```

检索到的 HTML 页面显示我们已成功登录系统，返回给我们的网页是用户资料/帐户管理页面。旧版本的电报账户页面用你的用户名迎接我们；然而，自从本书的第一次修订以来，这种情况已经发生了变化，我们不得不假设“我的帐户”标题是成功登录的充分指标。

现在让我们尝试访问注销页面，它应该将我们从站点中注销，从而有效地使我们之前检索到的会话 cookie 失效。我们还会检查检索到的实际 URL，因为我们可能会被重定向到与请求页面不同的页面:

```py
>>> url_logon = 'https://auth.telegraph.co.uk/sam-ui/login.htm'
>>> url_logoff = 'https://auth.telegraph.co.uk/sam-ui/logoff.htm'
>>> import urllib, urllib2
>>> data = urllib.urlencode({'email': 'user@example.com', 'password': 'secret'})
>>> opener = urllib2.build_opener(urllib2.HTTPCookieProcessor())
>>> urllib2.install_opener(opener)
>>> res = opener.open(url_logon, data)
>>> html_logon = res.read()
>>> print res.geturl()
'https://auth.telegraph.co.uk/customer-portal/myaccount/index.html'
>>> res.close()
>>> res = opener.open(url_logoff)
>>> html_logoff = res.read()
>>> res.geturl()
'http://www.telegraph.co.uk/'
>>> res.close()
```

现在让我们看看<title>标签是否是一个可以用来区分注册页面(意味着我们已经成功登录)和主登录页面(意味着我们已经成功注销)的标记:</title>

```py
>>> from BeautifulSoup import BeautifulSoup
>>> soup_logon = BeautifulSoup(html_logon)
>>> soup_logoff = BeautifulSoup(html_logoff)
>>> soup_logon.head.title
<title>My Account</title>
>>> soup_logoff.head.title
<title>The Telegraph - Telegraph online, Daily Telegraph, Sunday Telegraph - Telegraph</title>
>>>
```

事实上，这被证明是一个合理有效的测试。在我们提交身份验证表单后，我们将检查该 URL 是否与帐户管理 URL([https://auth . telegraph . co . uk/customer-portal/My Account/index . HTML](https://auth.telegraph.co.uk/customer-portal/myaccount/index.html))匹配，以及 HTML < title >标签是否包含关键字短语“我的帐户”类似地，当我们注销时，我们将检查我们是否被重定向到主页面，并且<标题>标签也被更改。

因此，我们有一种方法通过在 POST 数据请求中提交所需的信息来向网站验证我们自己。我们也可以使用相同的方法来提交大型表单。例如，我们可能想要建立一个自动检查来测试我们网站或评论系统的注册功能。

Nagios 系统的检查脚本与我们为导航测试编写的脚本非常相似。清单 8-5 显示了完整的脚本。

[***清单 8-5***](#_list5) 。站点登录/注销检查脚本

```py
#!/usr/bin/env python

import sys
import urllib2, urllib
import time
from BeautifulSoup import BeautifulSoup
from optparse import OptionParser

NAGIOS_OK = 0
NAGIOS_WARNING = 1
NAGIOS_CRITICAL = 2
WEBSITE_LOGON  = 'https://auth.telegraph.co.uk/sam-ui/login.htm'
WEBSITE_LOGOFF = 'https://auth.telegraph.co.uk/sam-ui/logoff.htm'
WEBSITE_USER = 'user@example.com'
WEBSITE_PASS = 'secret'

def test_logon_logoff():
    opener = urllib2.build_opener(urllib2.HTTPCookieProcessor())
    urllib2.install_opener(opener)
    data = urllib.urlencode({'email': WEBSITE_USER, 'password': WEBSITE_PASS})
    status = []
    try:
        # test logon
        result = opener.open(WEBSITE_LOGON, data)
        html_logon = result.read()
        soup_logon = BeautifulSoup(html_logon)
        logon_ok = validate_logon(soup_logon.head.title.text, result.geturl())
        result.close()
        # test logoff
        result = opener.open(WEBSITE_LOGOFF)
        html_logoff = result.read()
        soup_logoff = BeautifulSoup(html_logoff)
        logoff_ok = validate_logoff(soup_logoff.head.title.text, result.geturl())
        result.close()

        if logon_ok and logoff_ok:
            status = [NAGIOS_OK, 'Logon/logoff operation']
        else:
            status = [NAGIOS_CRITICAL, 'ERROR: Failed to logon and then logoff to the web site']
    except:
        status = [NAGIOS_CRITICAL, 'ERROR: Failure in the logon/logoff test']
    return status

def validate_logon(title, redirect_url):
    result = True
    if title.find('My Account') == -1:
        result = False
    if redirect_url != 'https://auth.telegraph.co.uk/customer-portal/myaccount/index.html':
        result = False
    return result

def validate_logoff(title, redirect_url):
    result = True
    if title.find('My Account') != -1:
        result = False
    if redirect_url != 'http://www.telegraph.co.uk/':
        result = False
    return result

def main():
    parser = OptionParser()
    parser.add_option('-w', dest='time_warn', default=3.8, help="Warning threshold in seconds, 
    defaul: %default")
    parser.add_option('-c', dest='time_crit', default=5.8, help="Critical threshold in seconds, 
    default: %default")
    (options, args) = parser.parse_args()
    if float(options.time_crit) < float(options.time_warn):
        options.time_warn = options.time_crit
    start = time.time()
    code, message = test_logon_logoff()
    elapsed = time.time() - start
    if code != 0:
        print message
        sys.exit(code)
    else:
        if elapsed < float(options.time_warn):
            print "OK: Performed %s sucessfully in %f seconds" % (message, elapsed)
            sys.exit(NAGIOS_OK)
        elif elapsed < float(options.time_crit):
            print "WARNING: Performed %s sucessfully in %f seconds" % (message, elapsed)
            sys.exit(NAGIOS_WARNING)
        else:
            print "CRITICAL: Performed %s sucessfully in %f seconds" % (message, elapsed)
            sys.exit(NAGIOS_CRITICAL)

if __name__ == '__main__':
    main()
```

我们需要将这个脚本添加到 commands.cfg 文件中，并在 Nagios 配置文件中创建适当的主机、主机组、服务和服务组定义，就像我们处理站点导航脚本一样。一旦我们添加了这个配置，我们重新启动 Nagios 进程，过一会儿我们应该会在 Nagios 控制台中看到 check 状态。

用请求模块简化 HTTP 客户端

在上一节中，我们使用标准 Python 库 urllib 和 urllib2 为我们发出所有 web 请求。不幸的是，这些库有时很难使用，API 可能会变得有点神秘，特别是在处理更复杂的任务时，比如维护会话 cookies。

对我们来说幸运的是，Kenneth Reitz 还发现这两个库使用起来相当痛苦，他非常沮丧，于是决定编写一个更好、更简单的 HTTP 客户端库版本。因此，在 2011 年初，他编写了 requests 库的初始版本，该版本非常受欢迎，因为它提供了一种更加优雅和高效的方式来处理 web 请求。

正在安装请求库

大多数 Linux 发行版的默认软件包中都有一个 requests 模块。例如，在 Fedora 系统上，您可以运行以下命令来安装它:

```py
$ sudo yum install python-requests
```

或者，您可以使用 PyPI 包:

```py
$ sudo pip install requests
```

requests 库依赖于另一个非标准库 urllib3，但它包含在软件包中，因此您不需要单独安装它。因为这些包是捆绑在一起的，所以您也不需要担心包的版本。

基本用法

请求模块如此受欢迎的原因之一是它不需要编写任何样板代码。我们需要做的就是导入模块并调用一个与 HTTP 方法名相匹配的帮助函数。下面是我们检索 BBC 新闻主网页并检查 HTML 文档大小的方法:

```py
>>> import requests
>>> result = requests.get('http://www.bbc.co.uk/news')
>>> len(result.text)
145858
>>>
```

同样容易的是，我们可以发出 POST、DELETE 和 HEAD 请求，因为它们都作为助手函数公开。例如，HEAD 请求可用于获取有关网页的元数据，而无需下载页面的全部内容:

```py
>>> result = requests.head('http://www.bbc.co.uk/news/')
>>> len(result.text)
0
>>> result.status_code
200
>>> for k, v in result.headers.iteritems():
...   print "%s: %s" % (k, v)
...
x-lb-nocache: true
content-length: 145858
content-type: text/html
content-language: en-GB
set-cookie: BBC-UID=f5933797120e92467907670df14f1beb2510f8d4b4f4d11e3a3144ee92a8ee880python-requests/2.3.0%20CPython/2.7.5%20Darwin/13.2.0; expires=Wed, 16-May-18 09:38:46 GMT; path=/; 
domain=.bbc.co.uk
expires: Sat, 17 May 2014 09:38:37 GMT
vary: X-CDN
server: Apache
connection: keep-alive
x-cache-hits: 9
x-cache-action: HIT
cache-control: private, max-age=0, must-revalidate
date: Sat, 17 May 2014 09:38:46 GMT
x-cache-age: 9
>>>
```

正如您所看到的，headers 属性允许我们访问 HTTP 响应头，非常类似于 urllib2 实现的 info()方法。但不是返回 httplib。HTTPMessage 类实例，headers 属性是一个简单的 Python 字典，我们可以直接迭代和操作它。

现在，如果你仔细看看上面的两个例子，你会注意到一些有趣的事情。一个是 content-length 头包含表示网页大小的数字。因此，您可以使用这些信息来检测网页内容的变化。您还应该注意到，当我使用 GET 方法检索网页时，我没有在 URL 后面附加“/”符号，而是使用 HEAD 方法。这只是因为 requests.get()函数跟踪所有重定向。您可以看到实际检索到的 URL 与我们请求的不同。响应对象还包含完整的重定向历史，因此您可以看到所有中间 URL:

```py
>>> result = requests.get('http://www.bbc.co.uk/news')
>>> result.url
u'http://www.bbc.co.uk/news/'
>>> result.history
[<Response [301]>]
>>> for r in result.history:
...   print r.url
...
http://www.bbc.co.uk/news
>>>
```

相反，requests.head()函数不遵循重定向:

```py
>>> result = requests.head('http://www.bbc.co.uk/news')
>>> result.url
u'http://www.bbc.co.uk/news'
>>> result.status_code
301
>>>
```

要登录到 Telegraph 网站，我们需要使用 HTTP POST 请求将表单数据(电子邮件和密码)发送到网站:

```py
>>> form_data = {'email': 'user@example.com', 'password': 'secret'}
>>> result = requests.post('https://auth.telegraph.co.uk/sam-ui/login.htm', data=form_data)
>>> result.url
u'https://auth.telegraph.co.uk/customer-portal/myaccount/index.html'
>>>
```

不过，我们还有一个小问题。在两次请求之间，网站不会记住我们，随后尝试检索帐户页面会将我们重定向回主登录页面:

```py
>>> form_data = {'email': 'user@example.com', 'password': 'secret'}
>>> result = requests.post('https://auth.telegraph.co.uk/sam-ui/login.htm', data=form_data)
>>> result.url
u'https://auth.telegraph.co.uk/customer-portal/myaccount/index.html'
>>> result = requests.get('https://auth.telegraph.co.uk/customer-portal/myaccount/index.html')
>>> result.url
u'https://auth.telegraph.co.uk/sam-ui/login.htm?redirectTo=http%3A%2F%2Fauth.telegraph.co.uk%2Fcustomer-portal%2Fmyaccount%2Findex.html&logintype=tmg'
>>>
```

众所周知，网站使用存储在 cookies 中的信息来识别我们的身份。对于 urllib 和 urllib2，我们必须编写大量样板代码来实现 cookie 持久性。requests 模块简化了这个过程——我们需要做的就是创建一个 Session 类的实例，并使用它来代替主 requests 模块:

```py
>>> session = requests.Session()
>>> form_data = {'email': 'user@example.com', 'password': 'secret'}
>>> result = session.post('https://auth.telegraph.co.uk/sam-ui/login.htm', data=form_data)
>>> result.url
u'https://auth.telegraph.co.uk/customer-portal/myaccount/index.html'
>>> result = session.get('https://auth.telegraph.co.uk/customer-portal/myaccount/index.html')
>>> result.url
u'https://auth.telegraph.co.uk/customer-portal/myaccount/index.html'
>>>
```

可以看到，requests 模块提供了一种更简洁的方式来处理高级 HTTP 协议操作。有关请求模块高级使用的更多信息，请访问:[http://requests.readthedocs.org/en/latest/](http://requests.readthedocs.org/en/latest/)。

重写站点登录检查脚本

现在让我们使用请求模块重写登录/注销检查脚本，如清单 8-6 : 所示

[***清单 8-6***](#_list6) 。使用请求模块的站点登录/注销检查脚本

```py
#!/usr/bin/env python

import sys
import urllib2, urllib
import time
import requests
from BeautifulSoup import BeautifulSoup
from optparse import OptionParser

NAGIOS_OK = 0
NAGIOS_WARNING = 1
NAGIOS_CRITICAL = 2
WEBSITE_LOGON  = 'https://auth.telegraph.co.uk/sam-ui/login.htm'
WEBSITE_LOGOFF = 'https://auth.telegraph.co.uk/sam-ui/logoff.htm'
WEBSITE_USER = 'user@example.com'
WEBSITE_PASS = 'secret'

def test_logon_logoff():
    session = requests.Session()
    form_data = {'email': WEBSITE_USER, 'password': WEBSITE_PASS}
    status = []
    try:
        # test logon
        result = session.post(WEBSITE_LOGON, data=form_data)
        html_logon = result.text
        soup_logon = BeautifulSoup(html_logon)
        logon_ok = validate_logon(soup_logon.head.title.text, result.url)
        # test logoff
        result = session.get(WEBSITE_LOGOFF)
        html_logoff = result.text
        soup_logoff = BeautifulSoup(html_logoff)
        logoff_ok = validate_logoff(soup_logoff.head.title.text, result.url)

        if logon_ok and logoff_ok:
            status = [NAGIOS_OK, 'Logon/logoff operation']
        else:
            status = [NAGIOS_CRITICAL,
                      'ERROR: Failed to logon and then logoff to the web site']
    except:
        status = [NAGIOS_CRITICAL, 'ERROR: Failure in the logon/logoff test']
        import traceback
        traceback.print_exc()
    return status

def validate_logon(title, redirect_url):
    result = True
    if title.find('My Account') == -1:
        result = False
    if redirect_url != 'https://auth.telegraph.co.uk/customer-portal/myaccount/index.html':
        result = False
    return result

def validate_logoff(title, redirect_url):
    result = True
    if title.find('My Account') != -1:
        result = False
    if redirect_url != 'http://www.telegraph.co.uk':
        result = False
    return result

def main():
    parser = OptionParser()
    parser.add_option('-w', dest='time_warn', default=3.8,
                      help="Warning threshold in seconds, defaul: %default")
    parser.add_option('-c', dest='time_crit', default=5.8,
                      help="Critical threshold in seconds, default: %default")
    (options, args) = parser.parse_args()
    if float(options.time_crit) < float(options.time_warn):
        options.time_warn = options.time_crit
    start = time.time()
    code, message = test_logon_logoff()
    elapsed = time.time() - start
    if code != 0:
        print message
        sys.exit(code)
    else:
        if elapsed < float(options.time_warn):
            print "OK: Performed %s sucessfully in %f seconds" % (message, elapsed)
            sys.exit(NAGIOS_OK)
        elif elapsed < float(options.time_crit):
            print "WARNING: Performed %s sucessfully in %f seconds" % (message, elapsed)
            sys.exit(NAGIOS_WARNING)
        else:
            print "CRITICAL: Performed %s sucessfully in %f seconds" % (message, elapsed)
            sys.exit(NAGIOS_CRITICAL)

if __name__ == '__main__':
    main()
```

摘要

在这一章中，我们已经了解了超越简单 HTTP 进程检查的网站监控脚本。这些测试模拟标准用户行为，并实际测试 web 应用逻辑。需要记住的要点:

*   您可以使用标准的 Python urllib2 模块来访问 web 内容。
*   urllib2 库提供了无缝管理 cookies 的附加处理程序。
*   如果可用，您可以使用请求模块来执行高级 HTTP 请求操作。
*   你可以用漂亮的 Soup 库解析 HTML 文档。
*   通过基于标准 UNIX 进程通信机制的 API，很容易将应用与 Nagios 监控系统集成在一起。
*   你可以在官方文档中找到关于 Nagios API 的详细信息，可以在[www.nagios.org/documentation/](http://www.nagios.org/documentation/)*找到。*